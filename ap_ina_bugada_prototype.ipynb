{
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N6dJfsi6D-rF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ap-ina-bugada-prototype"
      ],
      "metadata": {
        "id": "E6URpzqwIaKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Étape 1 — Préparation cartes BUGADA/JIRA (PROPRE, anti-leakage)\n",
        "- Lecture robuste (JSON / JSON dict / JSONL)\n",
        "- Normalisation cartes (Bugzilla, Jira, fallback)\n",
        "- Extraction desc_blob (description + 1er commentaire si dispo)\n",
        "- Détection leakage (status/résolution post-hoc) + raisons  [QC seulement]\n",
        "- QC: missingness.csv, duplicates.csv\n",
        "- Figures \"papier\" + copies -> paper_assets/\n",
        "- dataset_stats.json + DATACARD.md enrichi (provenance/licence)\n",
        "- feature_allowlist.txt (liste blanche anti-leakage)\n",
        "\"\"\"\n",
        "\n",
        "import os, json, sys, csv, hashlib, shutil\n",
        "from collections import Counter, defaultdict\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "# ============ RÉGLAGES (⚠️ adapte ces chemins) ============\n",
        "INPUT_PATH = \"/content/drive/MyDrive/bugdata/bugs.json\"\n",
        "OUT_DIR    = \"/content/drive/MyDrive/bugada_cards_clean\"\n",
        "LIMIT      = None  # None = pas de limite\n",
        "# ===========================================================\n",
        "\n",
        "# Métadonnées (modifie si tu n’es pas sur BMO)\n",
        "DATA_SOURCE_NAME = \"Bugzilla@Mozilla (BMO)\"\n",
        "DATA_SOURCE_URL  = \"https://bugzilla.mozilla.org\"\n",
        "DATA_ACCESS_DATE = \"2025-11-05\"\n",
        "DATA_LICENSE     = \"Mozilla Websites & Communications Terms of Use (voir mentions sur le site)\"\n",
        "DATA_LICENSE_URL = \"https://www.mozilla.org/en-US/about/legal/terms/mozilla/\"\n",
        "\n",
        "# ---------------- Utils ----------------\n",
        "def now_iso():\n",
        "    return datetime.now(timezone.utc).replace(microsecond=0).isoformat()\n",
        "\n",
        "def ensure_dir(p):\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "def safe_str(x, default=\"\"):\n",
        "    if x is None: return default\n",
        "    try:\n",
        "        return str(x)\n",
        "    except Exception:\n",
        "        return default\n",
        "\n",
        "def sha256_bytes(b: bytes) -> str:\n",
        "    return \"sha256:\" + hashlib.sha256(b).hexdigest()\n",
        "\n",
        "def sha256_file(path: str) -> str:\n",
        "    with open(path, \"rb\") as f:\n",
        "        return sha256_bytes(f.read())\n",
        "\n",
        "def sha256_obj(obj) -> str:\n",
        "    s = json.dumps(obj, ensure_ascii=False, sort_keys=True, separators=(\",\", \":\"))\n",
        "    return sha256_bytes(s.encode(\"utf-8\"))\n",
        "\n",
        "# ------------- Lecture robuste -------------\n",
        "def read_json_any(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        txt = f.read()\n",
        "\n",
        "    # JSON (liste/dict)\n",
        "    try:\n",
        "        data = json.loads(txt)\n",
        "        if isinstance(data, dict):\n",
        "            for key in (\"bugs\",\"issues\",\"data\",\"items\"):\n",
        "                if key in data and isinstance(data[key], list):\n",
        "                    print(f\"[LOAD] JSON dict avec liste '{key}' -> {len(data[key])} éléments\")\n",
        "                    return data[key]\n",
        "            print(\"[LOAD] JSON dict sans clé-liste standard -> 1 élément (dict)\")\n",
        "            return [data]\n",
        "        if isinstance(data, list):\n",
        "            print(f\"[LOAD] JSON liste -> {len(data)} éléments\")\n",
        "            return data\n",
        "    except Exception:\n",
        "        print(\"[LOAD] Pas un gros JSON, on tente JSONL…\")\n",
        "\n",
        "    # JSONL\n",
        "    items = []\n",
        "    for i, line in enumerate(txt.splitlines(), 1):\n",
        "        line = line.strip()\n",
        "        if not line: continue\n",
        "        try:\n",
        "            obj = json.loads(line)\n",
        "            if isinstance(obj, dict) and \"bug\" in obj and isinstance(obj[\"bug\"], dict):\n",
        "                obj = obj[\"bug\"]\n",
        "            items.append(obj)\n",
        "        except Exception:\n",
        "            continue\n",
        "    print(f\"[LOAD] JSONL -> {len(items)} lignes valides\")\n",
        "    return items\n",
        "\n",
        "# -------- Détection systèmes --------\n",
        "def is_jira(bug):\n",
        "    if \"key\" in bug: return True\n",
        "    if \"fields\" in bug and isinstance(bug[\"fields\"], dict): return True\n",
        "    if \"project\" in bug and (\"components\" in bug or \"Component\" in bug): return True\n",
        "    return False\n",
        "\n",
        "def is_bugzilla(bug):\n",
        "    return (\"id\" in bug or \"bug_id\" in bug) and (\"summary\" in bug or \"short_desc\" in bug or \"title\" in bug)\n",
        "\n",
        "# -------- Normalisation / util --------\n",
        "def norm_severity_generic(s):\n",
        "    s = (s or \"\").strip().lower()\n",
        "    repl = {\n",
        "        \"critical\":\"high\",\"blocker\":\"high\",\"major\":\"high\",\n",
        "        \"minor\":\"low\",\"trivial\":\"low\",\"normal\":\"medium\",\n",
        "        \"--\":\"unknown\",\"\": \"unknown\",\"none\":\"unknown\",\"n/a\":\"unknown\"\n",
        "    }\n",
        "    return repl.get(s, s or \"unknown\")\n",
        "\n",
        "def norm_severity_bugzilla(s):\n",
        "    s = (s or \"\").strip().lower()\n",
        "    m = {\"s1\":\"high\",\"s2\":\"high\",\"s3\":\"medium\",\"s4\":\"low\",\"s5\":\"low\"}\n",
        "    return m.get(s, norm_severity_generic(s))\n",
        "\n",
        "def compute_desc_len(summary, long_text, comments_len_chars=0):\n",
        "    return len(safe_str(summary)) + len(safe_str(long_text)) + int(comments_len_chars)\n",
        "\n",
        "# Hints (déterministes; UNIQUEMENT texte)\n",
        "REGRESSION_HINTS = [\"regression\",\"after update\",\"after upgrade\",\"after release\",\"after deploy\",\"since version\",\"introduced in\",\"since build\",\"since release\"]\n",
        "INFRA_HINTS      = [\"timeout\",\"latency\",\"slow\",\"slowdown\",\"performance\",\"5xx\",\"server error\",\"network\",\"intermittent\",\"connection reset\",\"502\",\"503\",\"bad gateway\"]\n",
        "INSUFF_HINTS     = [\"cannot reproduce\",\"need more info\",\"needinfo\",\"missing steps\",\"incomplete\",\"unconfirmed\",\"no steps\"]\n",
        "BUGZILLA_NEG     = {\"worksforme\",\"works as intended\",\"notabug\",\"invalid\",\"works for me\",\"duplicate\",\"moved\"}\n",
        "BUGZILLA_BSE     = {\"wontfix\",\"by design\",\"policy decision\"}\n",
        "\n",
        "def extract_keywords_from_text(*texts):\n",
        "    \"\"\"Calcule des mots-clés uniquement depuis le TEXTE (pas de status/résolution).\"\"\"\n",
        "    t = \" \".join([safe_str(x).lower() for x in texts if x])\n",
        "    kw = set()\n",
        "    if any(h in t for h in REGRESSION_HINTS): kw.add(\"regression\")\n",
        "    for h in INFRA_HINTS:\n",
        "        if h in t: kw.add(h.split()[0])  # \"latency\",\"timeout\",\"slow\",\"5xx\",\"network\",\"intermittent\",\"502\",\"503\",\"bad\"\n",
        "    for h in INSUFF_HINTS:\n",
        "        if h in t: kw.add(\"needinfo\")\n",
        "    for r in BUGZILLA_NEG:\n",
        "        if r in t: kw.add(\"falsepositive\")\n",
        "    for r in BUGZILLA_BSE:\n",
        "        if r in t: kw.add(\"wontfix\")\n",
        "    return sorted(kw)\n",
        "\n",
        "def has_security_indicator(*texts) -> bool:\n",
        "    t = \" \".join([safe_str(x).lower() for x in texts if x])\n",
        "    SEC = (\"security\",\"vulnerability\",\"xss\",\"csrf\",\"sql injection\")\n",
        "    return any(s in t for s in SEC)\n",
        "\n",
        "# -------- Extracteurs desc_blob --------\n",
        "def _first_nonempty(*candidates):\n",
        "    for c in candidates:\n",
        "        c = safe_str(c).strip()\n",
        "        if c:\n",
        "            return c\n",
        "    return \"\"\n",
        "\n",
        "def _extract_desc_from_bugzilla(bug):\n",
        "    long_text_fields = [\n",
        "        bug.get(\"description\"), bug.get(\"desc\"), bug.get(\"longdesc\"),\n",
        "        bug.get(\"long_desc\"), bug.get(\"raw_text\"), bug.get(\"text\"), bug.get(\"details\"),\n",
        "    ]\n",
        "    base = _first_nonempty(*long_text_fields)\n",
        "\n",
        "    first_comment = \"\"\n",
        "    comments_len_chars = 0\n",
        "    comments = bug.get(\"comments\")\n",
        "    if isinstance(comments, list) and comments:\n",
        "        for c in comments:\n",
        "            ctext = safe_str(c.get(\"text\") or c.get(\"raw_text\") or c.get(\"body\"))\n",
        "            comments_len_chars += len(ctext)\n",
        "        first_comment = safe_str(comments[0].get(\"text\") or comments[0].get(\"raw_text\") or comments[0].get(\"body\"))\n",
        "    elif isinstance(comments, dict):\n",
        "        arr = comments.get(\"comments\") or comments.get(\"data\") or []\n",
        "        if isinstance(arr, list) and arr:\n",
        "            for c in arr:\n",
        "                ctext = safe_str(c.get(\"text\") or c.get(\"raw_text\") or c.get(\"body\"))\n",
        "                comments_len_chars += len(ctext)\n",
        "            first_comment = safe_str(arr[0].get(\"text\") or arr[0].get(\"raw_text\") or arr[0].get(\"body\"))\n",
        "\n",
        "    desc_blob = base if base else first_comment\n",
        "    return desc_blob, comments_len_chars\n",
        "\n",
        "def _extract_desc_from_jira(bug):\n",
        "    fields = bug.get(\"fields\") if isinstance(bug.get(\"fields\"), dict) else {}\n",
        "    base = safe_str(fields.get(\"description\") or bug.get(\"description\") or \"\")\n",
        "    comments_len_chars = 0\n",
        "    if \"comment\" in fields and isinstance(fields[\"comment\"], dict):\n",
        "        arr = fields[\"comment\"].get(\"comments\") or []\n",
        "        if isinstance(arr, list):\n",
        "            for c in arr:\n",
        "                comments_len_chars += len(safe_str(c.get(\"body\")))\n",
        "            if not base and arr:\n",
        "                base = safe_str(arr[0].get(\"body\"))\n",
        "    return base, comments_len_chars\n",
        "\n",
        "# -------- Cartes --------\n",
        "CLOSED_STATUSES = {\"resolved\",\"closed\",\"verified\",\"done\",\"fixed\"}\n",
        "\n",
        "def _leakage_from_status_resolution(status, resolution):\n",
        "    \"\"\"QC : fuite potentielle si on injecte ça en features (on NE le fera pas).\"\"\"\n",
        "    reasons = []\n",
        "    st = safe_str(status).strip().lower()\n",
        "    res = safe_str(resolution).strip().upper()\n",
        "    if st in CLOSED_STATUSES:\n",
        "        reasons.append(f\"status={status}\")\n",
        "    if res:\n",
        "        reasons.append(f\"resolution={resolution}\")\n",
        "    return (len(reasons) > 0), reasons\n",
        "\n",
        "def jira_to_card(bug):\n",
        "    fields = bug.get(\"fields\") if isinstance(bug.get(\"fields\"), dict) else {}\n",
        "\n",
        "    key = safe_str(bug.get(\"key\") or bug.get(\"id\") or bug.get(\"ticket_id\") or \"\")\n",
        "    # project\n",
        "    if \"project\" in fields and isinstance(fields[\"project\"], dict) and fields[\"project\"].get(\"key\"):\n",
        "        project = fields[\"project\"][\"key\"]\n",
        "    else:\n",
        "        project = safe_str(bug.get(\"project\") or \"BUGS\")\n",
        "\n",
        "    # component\n",
        "    component = None\n",
        "    if \"components\" in fields and isinstance(fields[\"components\"], list) and fields[\"components\"]:\n",
        "        c0 = fields[\"components\"][0]\n",
        "        component = c0.get(\"name\") if isinstance(c0, dict) else str(c0)\n",
        "    if not component:\n",
        "        component = bug.get(\"component\") or bug.get(\"Component\") or \"General\"\n",
        "        if isinstance(component, list) and component:\n",
        "            component = component[0]\n",
        "    component = safe_str(component)\n",
        "\n",
        "    # created\n",
        "    created = fields.get(\"created\") or bug.get(\"created\") or bug.get(\"created_at\") or now_iso()\n",
        "    created = safe_str(created)\n",
        "\n",
        "    # status / severity / resolution\n",
        "    status = \"\"\n",
        "    if \"status\" in fields and isinstance(fields[\"status\"], dict):\n",
        "        status = fields[\"status\"].get(\"name\") or \"\"\n",
        "    status = status or bug.get(\"status\") or bug.get(\"status_current\") or \"unknown\"\n",
        "\n",
        "    sev = \"\"\n",
        "    if \"priority\" in fields and isinstance(fields[\"priority\"], dict):\n",
        "        sev = fields[\"priority\"].get(\"name\") or \"\"\n",
        "    sev = sev or bug.get(\"severity\") or \"unknown\"\n",
        "    severity = norm_severity_generic(sev)\n",
        "\n",
        "    resolution = \"\"\n",
        "    if isinstance(fields.get(\"resolution\"), dict):\n",
        "        resolution = fields[\"resolution\"].get(\"name\") or \"\"\n",
        "    else:\n",
        "        resolution = safe_str(bug.get(\"resolution\") or \"\")\n",
        "\n",
        "    # texte\n",
        "    summary = fields.get(\"summary\") or bug.get(\"summary\") or bug.get(\"title\") or \"\"\n",
        "    desc_blob, comments_len_chars = _extract_desc_from_jira(bug)\n",
        "    desc_len = compute_desc_len(summary, desc_blob, comments_len_chars)\n",
        "\n",
        "    # DERIVÉS — ***UNIQUEMENT TEXTE***\n",
        "    keywords = extract_keywords_from_text(summary, desc_blob)\n",
        "    security_flag = has_security_indicator(summary, desc_blob)\n",
        "    leakage_flag, leakage_reasons = _leakage_from_status_resolution(status, resolution)\n",
        "\n",
        "    # ids\n",
        "    ticket_id = bug.get(\"id\") or bug.get(\"ticket_id\")\n",
        "    ticket_id = safe_str(ticket_id) if ticket_id is not None else None\n",
        "    raw_hash = sha256_obj(bug)\n",
        "\n",
        "    return {\n",
        "        \"ticket_id\": ticket_id,\n",
        "        \"key\": key or (project + \"-\" + safe_str(ticket_id) if ticket_id else \"BUGS-AUTO\"),\n",
        "        \"created_at\": created,\n",
        "        \"project\": project or \"BUGS\",\n",
        "        \"component\": component or \"General\",\n",
        "        \"severity\": severity,\n",
        "        \"status_current\": safe_str(status),\n",
        "        \"title_len\": len(safe_str(summary)),\n",
        "        \"summary_len\": len(safe_str(summary)),\n",
        "        \"text_len\": len(safe_str(desc_blob)),\n",
        "        \"desc_len\": int(desc_len),\n",
        "        \"summary_text\": safe_str(summary),\n",
        "        \"desc_blob\": safe_str(desc_blob),\n",
        "        \"keywords\": keywords,               # OK (texte only)\n",
        "        \"security_flag\": bool(security_flag),\n",
        "        \"resolution\": safe_str(resolution), # QC only\n",
        "        \"leakage_flag\": bool(leakage_flag), # QC only\n",
        "        \"leakage_reasons\": leakage_reasons, # QC only\n",
        "        \"comments_len\": int(comments_len_chars),\n",
        "        \"recent_incidents_1h\": 0,\n",
        "        \"source_system\": \"jira\",\n",
        "        \"raw_sha256\": raw_hash\n",
        "    }\n",
        "\n",
        "def bugzilla_to_card(bug):\n",
        "    bid  = bug.get(\"id\") or bug.get(\"bug_id\") or bug.get(\"ticket_id\")\n",
        "    project = safe_str(bug.get(\"product\") or bug.get(\"project\") or \"BUGS\")\n",
        "\n",
        "    # component\n",
        "    comp = bug.get(\"component\")\n",
        "    if isinstance(comp, list) and comp:\n",
        "        comp = comp[0]\n",
        "    comp = safe_str(comp or \"General\")\n",
        "\n",
        "    # created\n",
        "    created = bug.get(\"creation_time\") or bug.get(\"created\") or bug.get(\"created_at\") or now_iso()\n",
        "    created = safe_str(created)\n",
        "\n",
        "    # texte\n",
        "    summary = bug.get(\"summary\") or bug.get(\"short_desc\") or bug.get(\"title\") or \"\"\n",
        "    desc_blob, comments_len_chars = _extract_desc_from_bugzilla(bug)\n",
        "    desc_len = compute_desc_len(summary, desc_blob, comments_len_chars)\n",
        "\n",
        "    # severity / status / resolution\n",
        "    severity   = norm_severity_bugzilla(bug.get(\"severity\"))\n",
        "    status     = safe_str(bug.get(\"status\") or bug.get(\"status_current\") or \"unknown\")\n",
        "    resolution = safe_str(bug.get(\"resolution\") or \"\")\n",
        "\n",
        "    # DERIVÉS — ***UNIQUEMENT TEXTE***\n",
        "    keywords = extract_keywords_from_text(summary, desc_blob)\n",
        "    security_flag = has_security_indicator(summary, desc_blob)\n",
        "    leakage_flag, leakage_reasons = _leakage_from_status_resolution(status, resolution)\n",
        "\n",
        "    # ids\n",
        "    key = safe_str(bug.get(\"key\") or (project + \"-\" + safe_str(bid) if bid is not None else \"BUG-AUTO\"))\n",
        "    ticket_id = safe_str(bid) if bid is not None else None\n",
        "    raw_hash = sha256_obj(bug)\n",
        "\n",
        "    return {\n",
        "        \"ticket_id\": ticket_id,\n",
        "        \"key\": key,\n",
        "        \"created_at\": created,\n",
        "        \"project\": project,\n",
        "        \"component\": comp,\n",
        "        \"severity\": severity,\n",
        "        \"status_current\": status,           # QC\n",
        "        \"title_len\": len(safe_str(summary)),\n",
        "        \"summary_len\": len(safe_str(summary)),\n",
        "        \"text_len\": len(safe_str(desc_blob)),\n",
        "        \"desc_len\": int(desc_len),\n",
        "        \"summary_text\": safe_str(summary),\n",
        "        \"desc_blob\": safe_str(desc_blob),\n",
        "        \"keywords\": keywords,               # OK (texte only)\n",
        "        \"security_flag\": bool(security_flag),\n",
        "        \"resolution\": resolution,           # QC\n",
        "        \"leakage_flag\": bool(leakage_flag), # QC\n",
        "        \"leakage_reasons\": leakage_reasons, # QC\n",
        "        \"comments_len\": int(comments_len_chars),\n",
        "        \"recent_incidents_1h\": 0,\n",
        "        \"source_system\": \"bugzilla\",\n",
        "        \"raw_sha256\": raw_hash\n",
        "    }\n",
        "\n",
        "def build_card_from_bug(bug):\n",
        "    if is_jira(bug):      return jira_to_card(bug)\n",
        "    if is_bugzilla(bug):  return bugzilla_to_card(bug)\n",
        "\n",
        "    # Fallback générique (texte only pour keywords)\n",
        "    project = safe_str(bug.get(\"project\") or bug.get(\"product\") or \"BUGS\")\n",
        "    comp = bug.get(\"component\") or bug.get(\"components\") or \"General\"\n",
        "    if isinstance(comp, list) and comp:\n",
        "        comp = comp[0]\n",
        "    comp = safe_str(comp)\n",
        "    created = safe_str(bug.get(\"created_at\") or bug.get(\"created\") or now_iso())\n",
        "    summary = bug.get(\"summary\") or bug.get(\"title\") or \"\"\n",
        "    desc = bug.get(\"description\") or bug.get(\"text\") or \"\"\n",
        "    desc_len = compute_desc_len(summary, desc, 0)\n",
        "    severity = norm_severity_generic(bug.get(\"severity\"))\n",
        "    status = safe_str(bug.get(\"status\") or bug.get(\"status_current\") or \"unknown\")\n",
        "    resolution = safe_str(bug.get(\"resolution\") or \"\")\n",
        "\n",
        "    kwords = extract_keywords_from_text(summary, desc)   # <- texte only\n",
        "    sec_flag = has_security_indicator(summary, desc)\n",
        "    leakage_flag, leakage_reasons = _leakage_from_status_resolution(status, resolution)\n",
        "\n",
        "    bid = bug.get(\"id\") or bug.get(\"bug_id\") or bug.get(\"ticket_id\")\n",
        "    key = safe_str(bug.get(\"key\") or (project + \"-\" + safe_str(bid) if bid is not None else \"BUG-AUTO\"))\n",
        "    ticket_id = safe_str(bid) if bid is not None else None\n",
        "    raw_hash = sha256_obj(bug)\n",
        "\n",
        "    return {\n",
        "        \"ticket_id\": ticket_id,\n",
        "        \"key\": key,\n",
        "        \"created_at\": created,\n",
        "        \"project\": project,\n",
        "        \"component\": comp,\n",
        "        \"severity\": severity,\n",
        "        \"status_current\": status,           # QC\n",
        "        \"title_len\": len(safe_str(summary)),\n",
        "        \"summary_len\": len(safe_str(summary)),\n",
        "        \"text_len\": len(safe_str(desc)),\n",
        "        \"desc_len\": int(desc_len),\n",
        "        \"summary_text\": safe_str(summary),\n",
        "        \"desc_blob\": safe_str(desc),\n",
        "        \"keywords\": kwords,                 # OK (texte only)\n",
        "        \"security_flag\": bool(sec_flag),\n",
        "        \"resolution\": resolution,           # QC\n",
        "        \"leakage_flag\": bool(leakage_flag), # QC\n",
        "        \"leakage_reasons\": leakage_reasons, # QC\n",
        "        \"comments_len\": 0,\n",
        "        \"recent_incidents_1h\": 0,\n",
        "        \"source_system\": \"generic\",\n",
        "        \"raw_sha256\": raw_hash\n",
        "    }\n",
        "\n",
        "# -------- QC: missingness & duplicates --------\n",
        "def write_missingness(out_dir, records):\n",
        "    fields = set()\n",
        "    for r in records: fields.update(r.keys())\n",
        "    fields = sorted(fields)\n",
        "    n = len(records)\n",
        "    rows = []\n",
        "    for f in fields:\n",
        "        missing = sum(1 for r in records if (f not in r or r[f] in (None, \"\", [], {})))\n",
        "        rows.append({\"field\": f, \"missing\": missing, \"missing_rate\": round(missing/n, 6) if n else 0.0})\n",
        "    path = os.path.join(out_dir, \"missingness.csv\")\n",
        "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as fw:\n",
        "        w = csv.DictWriter(fw, fieldnames=[\"field\",\"missing\",\"missing_rate\"])\n",
        "        w.writeheader(); w.writerows(rows)\n",
        "    return path\n",
        "\n",
        "def write_duplicates(out_dir, records):\n",
        "    by_hash = defaultdict(list)\n",
        "    for r in records:\n",
        "        by_hash[r.get(\"raw_sha256\",\"\")].append(r)\n",
        "    dupe_rows = []\n",
        "    for h, arr in by_hash.items():\n",
        "        if not h or len(arr) < 2: continue\n",
        "        for r in arr:\n",
        "            dupe_rows.append({\n",
        "                \"raw_sha256\": h,\n",
        "                \"key\": r.get(\"key\",\"\"),\n",
        "                \"ticket_id\": r.get(\"ticket_id\",\"\"),\n",
        "                \"created_at\": r.get(\"created_at\",\"\"),\n",
        "                \"summary_text\": r.get(\"summary_text\",\"\")[:140]\n",
        "            })\n",
        "    path = os.path.join(out_dir, \"duplicates.csv\")\n",
        "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as fw:\n",
        "        w = csv.DictWriter(fw, fieldnames=[\"raw_sha256\",\"key\",\"ticket_id\",\"created_at\",\"summary_text\"])\n",
        "        w.writeheader()\n",
        "        w.writerows(dupe_rows)\n",
        "    return path, len(dupe_rows)\n",
        "\n",
        "# -------- Figures (matplotlib, sans style/couleur forcés) --------\n",
        "def _plt():\n",
        "    import matplotlib\n",
        "    matplotlib.use(\"Agg\")\n",
        "    import matplotlib.pyplot as plt\n",
        "    return plt\n",
        "\n",
        "def fig_summary_len_hist(out_dir, records):\n",
        "    plt = _plt()\n",
        "    vals = [r.get(\"summary_len\",0) for r in records]\n",
        "    vals = [v for v in vals if isinstance(v, int)]\n",
        "    plt.figure(figsize=(10,4.5))\n",
        "    plt.hist(vals, bins=30)\n",
        "    plt.xlabel(\"summary_len\"); plt.ylabel(\"count\"); plt.title(\"Summary length histogram\")\n",
        "    p = os.path.join(out_dir, \"fig_summary_len_hist.png\"); plt.savefig(p, bbox_inches=\"tight\"); plt.close(); return p\n",
        "\n",
        "def fig_status_dist(out_dir, records):\n",
        "    plt = _plt()\n",
        "    c = Counter(safe_str(r.get(\"status_current\",\"unknown\")).upper() for r in records)\n",
        "    labs, vals = zip(*sorted(c.items(), key=lambda x: (-x[1], x[0]))) if c else ([],[])\n",
        "    plt.figure(figsize=(10,4.5)); plt.bar(labs, vals); plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.ylabel(\"count\"); plt.title(\"Status distribution\")\n",
        "    p = os.path.join(out_dir, \"fig_status_dist.png\"); plt.savefig(p, bbox_inches=\"tight\"); plt.close(); return p\n",
        "\n",
        "def fig_components_top10(out_dir, records):\n",
        "    plt = _plt()\n",
        "    c = Counter((safe_str(r.get(\"component\",\"\")).strip() or \"General\") for r in records)\n",
        "    items = sorted(c.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "    labs = [k for k,_ in items]; vals = [v for _,v in items]\n",
        "    plt.figure(figsize=(12,4.5)); plt.bar(labs, vals); plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.ylabel(\"count\"); plt.title(\"Top-10 components\")\n",
        "    p = os.path.join(out_dir, \"fig_components_top10.png\"); plt.savefig(p, bbox_inches=\"tight\"); plt.close(); return p\n",
        "\n",
        "def fig_created_months(out_dir, records):\n",
        "    plt = _plt()\n",
        "    def month_key(ts):\n",
        "        s = safe_str(ts);\n",
        "        return s[:7] if len(s)>=7 else \"unknown\"\n",
        "    c = Counter(month_key(r.get(\"created_at\",\"\")) for r in records)\n",
        "    items = sorted((k,v) for k,v in c.items() if k!=\"unknown\")\n",
        "    labs = [k for k,_ in items]; vals = [v for _,v in items]\n",
        "    plt.figure(figsize=(12,4.5)); plt.plot(range(len(vals)), vals, marker=\"o\")\n",
        "    plt.xticks(range(len(labs)), labs, rotation=45, ha=\"right\")\n",
        "    plt.ylabel(\"count\"); plt.title(\"Created per month\")\n",
        "    p = os.path.join(out_dir, \"fig_created_months.png\"); plt.savefig(p, bbox_inches=\"tight\"); plt.close(); return p\n",
        "\n",
        "def fig_missingness(out_dir, missingness_csv):\n",
        "    plt = _plt()\n",
        "    rows = []\n",
        "    with open(missingness_csv, newline=\"\", encoding=\"utf-8\") as f:\n",
        "        rdr = csv.DictReader(f)\n",
        "        for r in rdr:\n",
        "            rows.append((r[\"field\"], float(r[\"missing_rate\"])))\n",
        "    rows = sorted(rows, key=lambda x: x[1], reverse=True)[:20]\n",
        "    labs = [a for a,_ in rows]; vals = [b for _,b in rows]\n",
        "    plt.figure(figsize=(12,5)); plt.bar(labs, vals); plt.xticks(rotation=60, ha=\"right\")\n",
        "    plt.ylabel(\"missing_rate\"); plt.title(\"Top missingness\")\n",
        "    p = os.path.join(out_dir, \"fig_missingness.png\"); plt.savefig(p, bbox_inches=\"tight\"); plt.close(); return p\n",
        "\n",
        "def fig_leakage_rules(out_dir, records):\n",
        "    plt = _plt()\n",
        "    cc = Counter()\n",
        "    for r in records:\n",
        "        if r.get(\"leakage_flag\"):\n",
        "            for rea in r.get(\"leakage_reasons\",[]) or []: cc[rea]+=1\n",
        "    items = sorted(cc.items(), key=lambda x: x[1], reverse=True)\n",
        "    labs = [k for k,_ in items]; vals = [v for _,v in items]\n",
        "    plt.figure(figsize=(12,4.5))\n",
        "    if labs:\n",
        "        plt.bar(labs, vals); plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.ylabel(\"count\"); plt.title(\"Leakage reasons\")\n",
        "    p = os.path.join(out_dir, \"fig_leakage_rules.png\"); plt.savefig(p, bbox_inches=\"tight\"); plt.close(); return p\n",
        "\n",
        "# -------- Allowlist --------\n",
        "ALLOWLIST_FIELDS = [\n",
        "    # Texte court + dérivés non post-hoc\n",
        "    \"summary_text\",\"summary_len\",\"title_len\",\"desc_len\",\"text_len\",\n",
        "    # Métadonnées “early”\n",
        "    \"project\",\"component\",\"severity\",\"security_flag\",\"keywords\",\n",
        "    # Temps brut (sera binned en aval si besoin)\n",
        "    \"created_at\",\n",
        "    # EXCLUS explicitement en aval: status_current, resolution, leakage_flag, leakage_reasons\n",
        "]\n",
        "\n",
        "def write_allowlist(out_dir):\n",
        "    path = os.path.join(out_dir, \"feature_allowlist.txt\")\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"# Features autorisées (anti-leakage)\\n\")\n",
        "        for k in ALLOWLIST_FIELDS:\n",
        "            f.write(k+\"\\n\")\n",
        "        f.write(\"\\n# EXCLUS (post-hoc): status_current, resolution, leakage_flag, leakage_reasons\\n\")\n",
        "    return path\n",
        "\n",
        "# -------- DataCard & stats --------\n",
        "def write_dataset_stats(out_dir, records, input_sha, figs, missing_csv, dup_count):\n",
        "    stats = {\n",
        "        \"generated_at\": now_iso(),\n",
        "        \"data_source_name\": DATA_SOURCE_NAME,\n",
        "        \"data_source_url\": DATA_SOURCE_URL,\n",
        "        \"data_access_date\": DATA_ACCESS_DATE,\n",
        "        \"data_license\": DATA_LICENSE,\n",
        "        \"data_license_url\": DATA_LICENSE_URL,\n",
        "        \"input_path_sha256\": input_sha,\n",
        "        \"cards\": len(records),\n",
        "        \"source_system_counts\": dict(Counter(r.get(\"source_system\",\"\") for r in records)),\n",
        "        \"desc_blob_empty_rate\": round(sum(1 for r in records if not r.get(\"desc_blob\"))/len(records), 6) if records else 0.0,\n",
        "        \"leakage_rate\": round(sum(1 for r in records if r.get(\"leakage_flag\"))/len(records), 6) if records else 0.0,\n",
        "        \"figures\": [os.path.basename(p) for p in figs if p],\n",
        "        \"missingness_csv\": os.path.basename(missing_csv),\n",
        "        \"duplicates_count_rows\": dup_count\n",
        "    }\n",
        "    p = os.path.join(out_dir, \"dataset_stats.json\")\n",
        "    with open(p, \"w\", encoding=\"utf-8\") as fw:\n",
        "        json.dump(stats, fw, ensure_ascii=False, indent=2)\n",
        "    return p, stats\n",
        "\n",
        "def write_datacard(out_dir, stats):\n",
        "    md = f\"\"\"# DataCard — BugAda Cards (Étape 1, PROPRE)\n",
        "\n",
        "- **Généré** : {stats.get('generated_at')}\n",
        "- **Source** : {stats.get('data_source_name')} — {stats.get('data_source_url')}\n",
        "- **Date d'accès** : {stats.get('data_access_date')}\n",
        "- **SHA256 entrée** : {stats.get('input_path_sha256')}\n",
        "- **Nombre de cartes** : {stats.get('cards')}\n",
        "- **Leakage rate (QC)** : {stats.get('leakage_rate')}\n",
        "- **% desc_blob vides** : {stats.get('desc_blob_empty_rate')}\n",
        "- **Systèmes** : {stats.get('source_system_counts')}\n",
        "- **Licence** : {stats.get('data_license')}  ({stats.get('data_license_url')})\n",
        "\n",
        "## Politique anti-leakage\n",
        "- Toute feature post-hoc (`status_current`, `resolution`) est **exclue** des features.\n",
        "- Publication d’une **liste blanche** : `feature_allowlist.txt`.\n",
        "\n",
        "## QC\n",
        "- `missingness.csv` (taux de champs manquants ; figure jointe).\n",
        "- `duplicates.csv` (doublons exacts par `raw_sha256`) — politique: *keep-first*.\n",
        "\n",
        "## Figures pour papier\n",
        "- `fig_summary_len_hist.png`\n",
        "- `fig_created_months.png`\n",
        "- `fig_components_top10.png`\n",
        "- `fig_missingness.png`\n",
        "- `fig_leakage_rules.png`\n",
        "\n",
        "## Note\n",
        "Si `desc_blob` est vide dans la source, les modèles en aval se contenteront du **titre** + métadonnées *non post-hoc*. C’est attendu et documenté.\n",
        "\"\"\"\n",
        "    p = os.path.join(out_dir, \"DATACARD.md\")\n",
        "    with open(p, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(md)\n",
        "    return p\n",
        "\n",
        "# -------- Main conversion --------\n",
        "def convert_to_cards(input_path, out_dir, limit=None):\n",
        "    ensure_dir(out_dir)\n",
        "    bugs = read_json_any(input_path)\n",
        "    if not isinstance(bugs, list) or len(bugs) == 0:\n",
        "        print(\"[ERR] Aucune donnée lisible.\"); sys.exit(1)\n",
        "    if len(bugs) == 1:\n",
        "        print(\"[WARN] Seulement 1 élément détecté. Vérifie le format d'entrée.\")\n",
        "\n",
        "    out_jsonl = os.path.join(out_dir, \"episodes_raw.jsonl\")\n",
        "    out_sample = os.path.join(out_dir, \"episodes_raw.sample.json\")\n",
        "    n = 0; sample = []; cards = []; c_desc_blob_empty = 0\n",
        "\n",
        "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as fw:\n",
        "        for bug in bugs:\n",
        "            try:\n",
        "                card = build_card_from_bug(bug)\n",
        "            except Exception:\n",
        "                continue\n",
        "            fw.write(json.dumps(card, ensure_ascii=False) + \"\\n\")\n",
        "            cards.append(card)\n",
        "            if not card.get(\"desc_blob\"): c_desc_blob_empty += 1\n",
        "            if len(sample) < 5: sample.append(card)\n",
        "            n += 1\n",
        "            if limit and n >= limit: break\n",
        "\n",
        "    with open(out_sample, \"w\", encoding=\"utf-8\") as fs:\n",
        "        json.dump(sample, fs, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # Aperçu console (3 cartes)\n",
        "    print(\"Aperçu de 3 cartes:\")\n",
        "    for r in cards[:3]:\n",
        "        print(json.dumps(r, ensure_ascii=False))\n",
        "\n",
        "    print(f\"[STATS] cartes: {n}\")\n",
        "    if n: print(f\"[STATS] %desc_blob vide (global): {c_desc_blob_empty/n:.2%}\")\n",
        "\n",
        "    # QC\n",
        "    miss_csv = write_missingness(out_dir, cards)\n",
        "    dup_csv, dup_rows = write_duplicates(out_dir, cards)\n",
        "\n",
        "    # Figures (papier)\n",
        "    figs = []\n",
        "    figs.append(fig_summary_len_hist(out_dir, cards))\n",
        "    figs.append(fig_status_dist(out_dir, cards))\n",
        "    figs.append(fig_components_top10(out_dir, cards))\n",
        "    figs.append(fig_created_months(out_dir, cards))\n",
        "    figs.append(fig_missingness(out_dir, miss_csv))\n",
        "    figs.append(fig_leakage_rules(out_dir, cards))\n",
        "    print(f\"[FIG] Graphiques écrits dans: {out_dir}\")\n",
        "\n",
        "    # Paper assets\n",
        "    paper_dir = os.path.join(out_dir, \"paper_assets\"); ensure_dir(paper_dir)\n",
        "    for p in figs:\n",
        "        if p: shutil.copy2(p, os.path.join(paper_dir, os.path.basename(p)))\n",
        "    print(f\"[PAPER] assets copiés -> {paper_dir}\")\n",
        "\n",
        "    # Allowlist\n",
        "    allow_path = write_allowlist(out_dir)\n",
        "\n",
        "    # Stats + datacard\n",
        "    input_sha = sha256_file(input_path)\n",
        "    stats_path, stats = write_dataset_stats(out_dir, cards, input_sha, figs, miss_csv, dup_rows)\n",
        "    dc_path = write_datacard(out_dir, stats)\n",
        "\n",
        "    print(f\"- JSONL    : {out_jsonl}\")\n",
        "    print(f\"- Échantill: {out_sample}\")\n",
        "    print(f\"[QC] missingness.csv | duplicates.csv écrits.\")\n",
        "    print(f\"[STATS] dataset_stats.json & DATACARD.md écrits.\")\n",
        "    print(f\"[ALLOW] {allow_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    convert_to_cards(INPUT_PATH, OUT_DIR, LIMIT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T84dTOA2Ib9i",
        "outputId": "75c922f7-d74d-43f2-9a85-89afc018f583"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOAD] JSON dict avec liste 'bugs' -> 200 éléments\n",
            "Aperçu de 3 cartes:\n",
            "{\"ticket_id\": \"10954\", \"key\": \"BUGS-10954\", \"created_at\": \"1999-07-30T22:55:51Z\", \"project\": \"BUGS\", \"component\": \"Settings UI\", \"severity\": \"medium\", \"status_current\": \"RESOLVED\", \"title_len\": 46, \"summary_len\": 46, \"text_len\": 0, \"desc_len\": 46, \"summary_text\": \"Dialup properties needs to be exposed in prefs\", \"desc_blob\": \"\", \"keywords\": [], \"security_flag\": false, \"resolution\": \"WONTFIX\", \"leakage_flag\": true, \"leakage_reasons\": [\"status=RESOLVED\", \"resolution=WONTFIX\"], \"comments_len\": 0, \"recent_incidents_1h\": 0, \"source_system\": \"bugzilla\", \"raw_sha256\": \"sha256:5492c7d87c2db3a7ae19f78785cbd98016893b657a978cf32897b2f445d085a7\"}\n",
            "{\"ticket_id\": \"14871\", \"key\": \"BUGS-14871\", \"created_at\": \"1999-09-24T21:49:34Z\", \"project\": \"BUGS\", \"component\": \"General\", \"severity\": \"low\", \"status_current\": \"RESOLVED\", \"title_len\": 27, \"summary_len\": 27, \"text_len\": 0, \"desc_len\": 27, \"summary_text\": \"[Find] Find whole word only\", \"desc_blob\": \"\", \"keywords\": [], \"security_flag\": false, \"resolution\": \"DUPLICATE\", \"leakage_flag\": true, \"leakage_reasons\": [\"status=RESOLVED\", \"resolution=DUPLICATE\"], \"comments_len\": 0, \"recent_incidents_1h\": 0, \"source_system\": \"bugzilla\", \"raw_sha256\": \"sha256:1fb2937dfb864d3601813d7f002b50a1502bc1a5d87ecfb0f605b188af082bf6\"}\n",
            "{\"ticket_id\": \"19118\", \"key\": \"BUGS-19118\", \"created_at\": \"1999-11-17T22:58:26Z\", \"project\": \"BUGS\", \"component\": \"Settings UI\", \"severity\": \"medium\", \"status_current\": \"RESOLVED\", \"title_len\": 62, \"summary_len\": 62, \"text_len\": 0, \"desc_len\": 62, \"summary_text\": \"Plug-In Manager (ui for choosing mimetype-plugin associations)\", \"desc_blob\": \"\", \"keywords\": [], \"security_flag\": false, \"resolution\": \"WONTFIX\", \"leakage_flag\": true, \"leakage_reasons\": [\"status=RESOLVED\", \"resolution=WONTFIX\"], \"comments_len\": 0, \"recent_incidents_1h\": 0, \"source_system\": \"bugzilla\", \"raw_sha256\": \"sha256:5cb86d204ac27b9ae56de1cbbaa0d5ffd2717e5b54a44b19ebfe55308bad2d25\"}\n",
            "[STATS] cartes: 200\n",
            "[STATS] %desc_blob vide (global): 100.00%\n",
            "[FIG] Graphiques écrits dans: /content/drive/MyDrive/bugada_cards_clean\n",
            "[PAPER] assets copiés -> /content/drive/MyDrive/bugada_cards_clean/paper_assets\n",
            "- JSONL    : /content/drive/MyDrive/bugada_cards_clean/episodes_raw.jsonl\n",
            "- Échantill: /content/drive/MyDrive/bugada_cards_clean/episodes_raw.sample.json\n",
            "[QC] missingness.csv | duplicates.csv écrits.\n",
            "[STATS] dataset_stats.json & DATACARD.md écrits.\n",
            "[ALLOW] /content/drive/MyDrive/bugada_cards_clean/feature_allowlist.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Étape 2 — Silver labels (BugAda)"
      ],
      "metadata": {
        "id": "ZGNuLJG-k0cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Étape 2 — Silver labels (BugAda-BMO)\n",
        "Entrée  : episodes_raw.jsonl (Étape 1, BMO)\n",
        "Sorties :\n",
        "  - episodes_with_silver.jsonl\n",
        "  - silver_labels.csv\n",
        "  - silver_stats.json\n",
        "  - fig_silver_dist.png\n",
        "  - fig_silver_coverage.png\n",
        "  - fig_labeled_ratio_by_component_top10.png\n",
        "  - fig_tickets_per_year.png\n",
        "\"\"\"\n",
        "\n",
        "import os, json, csv\n",
        "from collections import Counter\n",
        "\n",
        "# ====== CHEMINS (BMO uniquement pour l’instant) ======\n",
        "IN_JSONL  = \"/content/drive/MyDrive/bugada_cards_clean/episodes_raw.jsonl\"\n",
        "OUT_DIR   = \"/content/drive/MyDrive/cleansilver_bmo\"\n",
        "\n",
        "OUT_JSONL = os.path.join(OUT_DIR, \"episodes_with_silver.jsonl\")\n",
        "OUT_CSV   = os.path.join(OUT_DIR, \"silver_labels.csv\")\n",
        "OUT_STATS = os.path.join(OUT_DIR, \"silver_stats.json\")\n",
        "# =====================================================\n",
        "\n",
        "def ensure_dir(p):\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "def silver_label_from_card(card) -> str | None:\n",
        "    \"\"\"\n",
        "    Règles déterministes pour attribuer un label i:...\n",
        "    (OK d'utiliser status + resolution ici : c'est pour les labels, pas pour les features du modèle)\n",
        "    \"\"\"\n",
        "    r   = (card.get(\"resolution\") or \"\").lower().strip()\n",
        "    st  = (card.get(\"status_current\") or \"\").lower().strip()\n",
        "    txt = \" \".join([\n",
        "        str(card.get(\"summary_text\") or \"\"),\n",
        "        str(card.get(\"desc_blob\") or \"\"),\n",
        "        st, r,\n",
        "        \" \".join(card.get(\"keywords\") or []),\n",
        "    ]).lower()\n",
        "    sev = (card.get(\"severity\") or \"unknown\").lower()\n",
        "\n",
        "    # 1) Faux positifs\n",
        "    if r in {\"invalid\",\"worksforme\",\"works as intended\",\"notabug\",\"duplicate\",\"moved\"}:\n",
        "        return \"i:false_positive\"\n",
        "\n",
        "    # 2) Effet de bord business\n",
        "    if r in {\"wontfix\",\"by design\",\"policy decision\"}:\n",
        "        return \"i:business_side_effect\"\n",
        "\n",
        "    # 3) Info insuffisante\n",
        "    if (st in {\"needinfo\",\"unconfirmed\"} or\n",
        "        any(k in txt for k in [\"cannot reproduce\",\"missing steps\",\"need more info\",\n",
        "                               \"incomplete\",\"no steps\"])):\n",
        "        return \"i:insufficient_info\"\n",
        "\n",
        "    # 4) Sécurité\n",
        "    if any(k in txt for k in [\"xss\",\"csrf\",\"vulnerability\",\"sql injection\"]):\n",
        "        return \"i:security_threat\"\n",
        "\n",
        "    # 5) Régression de release\n",
        "    if any(k in txt for k in [\"after update\",\"after upgrade\",\"after release\",\n",
        "                              \"after deploy\",\"introduced in\",\"since version\",\n",
        "                              \"since build\",\"since release\",\"regression\"]):\n",
        "        return \"i:release_regression\"\n",
        "\n",
        "    # 6) Instabilité infra\n",
        "    if any(k in txt for k in [\"timeout\",\"latency\",\"slowdown\",\"slow\",\"5xx\",\n",
        "                              \"server error\",\"network\",\"intermittent\",\n",
        "                              \"connection reset\",\"502\",\"503\",\"bad gateway\"]):\n",
        "        return \"i:infra_instability\"\n",
        "\n",
        "    # 7) Petites dégradations\n",
        "    if sev in {\"low\",\"trivial\"}:\n",
        "        return \"i:minor_degradation\"\n",
        "\n",
        "    return None  # pas de label\n",
        "\n",
        "def _year_from_ts(ts: str) -> str | None:\n",
        "    if not ts:\n",
        "        return None\n",
        "    return ts[:4] if len(ts) >= 4 and ts[:4].isdigit() else None\n",
        "\n",
        "# --- Figures (matplotlib simple, sans style forcé) ---\n",
        "def _plt():\n",
        "    import matplotlib\n",
        "    matplotlib.use(\"Agg\")\n",
        "    import matplotlib.pyplot as plt\n",
        "    return plt\n",
        "\n",
        "def _plot_bar(counter: Counter, title: str, xlabel: str, out_png: str, rotation=45):\n",
        "    plt = _plt()\n",
        "    items = list(counter.items())\n",
        "    labels = [k for k,_ in items]\n",
        "    values = [v for _,v in items]\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.bar(range(len(items)), values)\n",
        "    plt.xticks(range(len(items)), labels, rotation=rotation, ha=\"right\")\n",
        "    plt.title(title); plt.xlabel(xlabel); plt.ylabel(\"count\")\n",
        "    plt.tight_layout(); plt.savefig(out_png, dpi=160); plt.close()\n",
        "\n",
        "def _plot_simple(values_dict: dict, title: str, xlabel: str, out_png: str, rotation=0):\n",
        "    _plot_bar(Counter(values_dict), title, xlabel, out_png, rotation=rotation)\n",
        "\n",
        "# ----------------- MAIN -----------------\n",
        "def main():\n",
        "    ensure_dir(OUT_DIR)\n",
        "\n",
        "    n = 0\n",
        "    n_labeled = 0\n",
        "    dist = Counter()\n",
        "    sev_c = Counter()\n",
        "    comp_total = Counter()\n",
        "    comp_labeled = Counter()\n",
        "    year_c = Counter()\n",
        "\n",
        "    with open(IN_JSONL, \"r\", encoding=\"utf-8\") as fr, \\\n",
        "         open(OUT_JSONL, \"w\", encoding=\"utf-8\") as fw, \\\n",
        "         open(OUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as fc:\n",
        "\n",
        "        cw = csv.writer(fc)\n",
        "        cw.writerow([\"episode_id\",\"silver_label\"])\n",
        "\n",
        "        for line in fr:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            card = json.loads(line)\n",
        "            n += 1\n",
        "\n",
        "            # stats auxiliaires\n",
        "            sev_c[(card.get(\"severity\") or \"unknown\").lower()] += 1\n",
        "            comp = card.get(\"component\") or \"General\"\n",
        "            comp_total[comp] += 1\n",
        "            y = _year_from_ts(card.get(\"created_at\",\"\"))\n",
        "            if y:\n",
        "                year_c[y] += 1\n",
        "\n",
        "            # label silver\n",
        "            label = silver_label_from_card(card)\n",
        "            if label:\n",
        "                n_labeled += 1\n",
        "                dist[label] += 1\n",
        "                comp_labeled[comp] += 1\n",
        "                cw.writerow([card.get(\"key\") or card.get(\"ticket_id\"), label])\n",
        "\n",
        "            card[\"_silver_label\"] = label  # peut être None\n",
        "            fw.write(json.dumps(card, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    coverage = (n_labeled / n) if n else 0.0\n",
        "    print(f\"[SILVER] épisodes: {n}\")\n",
        "    print(f\"[SILVER] couverts: {n_labeled}  ({coverage:.2%})\")\n",
        "    if dist:\n",
        "        print(\"[SILVER] distribution:\")\n",
        "        for k,v in sorted(dist.items(), key=lambda kv: (-kv[1], kv[0])):\n",
        "            print(f\"  - {k}: {v}\")\n",
        "    print(f\"- JSONL (avec labels) : {OUT_JSONL}\")\n",
        "    print(f\"- CSV labels          : {OUT_CSV}\")\n",
        "\n",
        "    # ----- Stats JSON -----\n",
        "    stats = {\n",
        "        \"episodes\": n,\n",
        "        \"labeled\": n_labeled,\n",
        "        \"coverage\": round(coverage, 4),\n",
        "        \"label_distribution\": dict(dist),\n",
        "        \"severity_distribution\": dict(sev_c),\n",
        "        \"component_total_top10\": dict(Counter(dict(comp_total)).most_common(10)),\n",
        "        \"component_labeled_top10\": dict(Counter(dict(comp_labeled)).most_common(10)),\n",
        "        \"year_counts\": dict(sorted(year_c.items()))\n",
        "    }\n",
        "    with open(OUT_STATS, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(stats, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # ----- Figures -----\n",
        "    # 1) Distribution des labels\n",
        "    _plot_bar(dist, \"Silver label distribution\", \"label\",\n",
        "              os.path.join(OUT_DIR, \"fig_silver_dist.png\"))\n",
        "    # 2) Couverture (labeled vs non-labeled)\n",
        "    cov_dict = {\"labeled\": n_labeled, \"unlabeled\": max(0, n - n_labeled)}\n",
        "    _plot_simple(cov_dict, \"Silver coverage\", \"class\",\n",
        "                 os.path.join(OUT_DIR, \"fig_silver_coverage.png\"), rotation=0)\n",
        "    # 3) Ratio de labellisation par composant (Top-10 composants par volume)\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        top10 = [c for c,_ in Counter(dict(comp_total)).most_common(10)]\n",
        "        ratios = []\n",
        "        for c in top10:\n",
        "            tot = comp_total[c]\n",
        "            lab = comp_labeled.get(c, 0)\n",
        "            ratios.append((lab / tot) if tot else 0.0)\n",
        "        plt.figure(figsize=(10,5))\n",
        "        plt.bar(range(len(top10)), ratios)\n",
        "        plt.xticks(range(len(top10)), top10, rotation=45, ha=\"right\")\n",
        "        plt.title(\"Labeled ratio by component (top-10 by volume)\")\n",
        "        plt.xlabel(\"component\"); plt.ylabel(\"labeled_ratio\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(OUT_DIR,\n",
        "                                 \"fig_labeled_ratio_by_component_top10.png\"),\n",
        "                    dpi=160)\n",
        "        plt.close()\n",
        "    except Exception as e:\n",
        "        print(\"[FIG][WARN] component ratio:\", e)\n",
        "\n",
        "    # 4) Tickets par année (qualité temporelle)\n",
        "    _plot_bar(Counter(dict(year_c)), \"Tickets per year\", \"year\",\n",
        "              os.path.join(OUT_DIR, \"fig_tickets_per_year.png\"), rotation=0)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "uCVUWTF3k4JE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ea4228c-0c7a-4d3f-e233-81487680c0d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SILVER] épisodes: 200\n",
            "[SILVER] couverts: 68  (34.00%)\n",
            "[SILVER] distribution:\n",
            "  - i:false_positive: 29\n",
            "  - i:business_side_effect: 19\n",
            "  - i:minor_degradation: 13\n",
            "  - i:insufficient_info: 6\n",
            "  - i:infra_instability: 1\n",
            "- JSONL (avec labels) : /content/drive/MyDrive/cleansilver_bmo/episodes_with_silver.jsonl\n",
            "- CSV labels          : /content/drive/MyDrive/cleansilver_bmo/silver_labels.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Étape 3 — Splits DEV / HOLDOUT-H (make_splits_dev_holdout)"
      ],
      "metadata": {
        "id": "5c9maoP-vXsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Étape 3 — Splits DEV / HOLDOUT-H (BMO)\n",
        "Entrée  : episodes_with_silver.jsonl (Étape 2)\n",
        "Sorties :\n",
        "  - /content/drive/MyDrive/splits_bmo/episodes_dev.jsonl\n",
        "  - /content/drive/MyDrive/splits_bmo/episodes_holdoutH.jsonl\n",
        "  - /content/drive/MyDrive/splits_bmo/splits.csv\n",
        "\"\"\"\n",
        "\n",
        "import os, json, csv, random\n",
        "from collections import Counter\n",
        "\n",
        "# ========= CHEMINS À ADAPTER SI BESOIN =========\n",
        "IN_JSONL = \"/content/drive/MyDrive/cleansilver_bmo/episodes_with_silver.jsonl\"\n",
        "OUT_DIR  = \"/content/drive/MyDrive/splits_bmo\"\n",
        "# =============================================\n",
        "\n",
        "DEV_RATIO = 0.785   # ~157/200 pour BMO\n",
        "SEED      = 2025\n",
        "random.seed(SEED)\n",
        "\n",
        "def ensure_dir(p: str):\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "def load_cards(path: str):\n",
        "    cards = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            try:\n",
        "                obj = json.loads(line)\n",
        "                cards.append(obj)\n",
        "            except Exception:\n",
        "                continue\n",
        "    return cards\n",
        "\n",
        "def main():\n",
        "    ensure_dir(OUT_DIR)\n",
        "\n",
        "    cards = load_cards(IN_JSONL)\n",
        "    n = len(cards)\n",
        "    if n == 0:\n",
        "        print(\"[ERR] Aucun épisode lu dans\", IN_JSONL)\n",
        "        return\n",
        "\n",
        "    # id d’épisode pour traçabilité\n",
        "    def eid(c):\n",
        "        return str(c.get(\"key\") or c.get(\"ticket_id\") or \"\")\n",
        "\n",
        "    # mélange déterministe\n",
        "    idx = list(range(n))\n",
        "    random.shuffle(idx)\n",
        "\n",
        "    dev_count = int(round(DEV_RATIO * n))\n",
        "    dev_idx   = set(idx[:dev_count])\n",
        "    h_idx     = set(idx[dev_count:])\n",
        "\n",
        "    out_dev_path = os.path.join(OUT_DIR, \"episodes_dev.jsonl\")\n",
        "    out_h_path   = os.path.join(OUT_DIR, \"episodes_holdoutH.jsonl\")\n",
        "    splits_csv   = os.path.join(OUT_DIR, \"splits.csv\")\n",
        "\n",
        "    # stats de labels\n",
        "    dist_all = Counter()\n",
        "    dist_dev = Counter()\n",
        "    dist_h   = Counter()\n",
        "\n",
        "    with open(out_dev_path, \"w\", encoding=\"utf-8\") as f_dev, \\\n",
        "         open(out_h_path,   \"w\", encoding=\"utf-8\") as f_h, \\\n",
        "         open(splits_csv,   \"w\", newline=\"\", encoding=\"utf-8\") as f_csv:\n",
        "\n",
        "        cw = csv.writer(f_csv)\n",
        "        cw.writerow([\"episode_id\", \"split\"])\n",
        "\n",
        "        for i, c in enumerate(cards):\n",
        "            lab = c.get(\"_silver_label\") or \"NONE\"\n",
        "            dist_all[lab] += 1\n",
        "\n",
        "            if i in dev_idx:\n",
        "                f_dev.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n",
        "                dist_dev[lab] += 1\n",
        "                cw.writerow([eid(c), \"DEV\"])\n",
        "            else:\n",
        "                f_h.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n",
        "                dist_h[lab] += 1\n",
        "                cw.writerow([eid(c), \"H\"])\n",
        "\n",
        "    print(f\"[SPLIT] total: {n} | DEV: {len(dev_idx)} | HOLDOUT-H: {len(h_idx)}\")\n",
        "    print(\"[SPLIT] label dist (ALL):\", dict(dist_all))\n",
        "    print(\"[SPLIT] label dist (DEV):\", dict(dist_dev))\n",
        "    print(\"[SPLIT] label dist (H  ):\", dict(dist_h))\n",
        "    print(\"- DEV       :\", out_dev_path)\n",
        "    print(\"- HOLDOUT-H :\", out_h_path)\n",
        "    print(\"- SPLITS    :\", splits_csv)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZfaeTZpvbz1",
        "outputId": "36672638-47c5-4306-8211-d6521a187157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SPLIT] total: 200 | DEV: 157 | HOLDOUT-H: 43\n",
            "[SPLIT] label dist (ALL): {'i:business_side_effect': 19, 'i:false_positive': 29, 'NONE': 132, 'i:insufficient_info': 6, 'i:minor_degradation': 13, 'i:infra_instability': 1}\n",
            "[SPLIT] label dist (DEV): {'i:false_positive': 23, 'i:business_side_effect': 16, 'NONE': 100, 'i:insufficient_info': 5, 'i:minor_degradation': 12, 'i:infra_instability': 1}\n",
            "[SPLIT] label dist (H  ): {'i:business_side_effect': 3, 'NONE': 32, 'i:false_positive': 6, 'i:insufficient_info': 1, 'i:minor_degradation': 1}\n",
            "- DEV       : /content/drive/MyDrive/splits_bmo/episodes_dev.jsonl\n",
            "- HOLDOUT-H : /content/drive/MyDrive/splits_bmo/episodes_holdoutH.jsonl\n",
            "- SPLITS    : /content/drive/MyDrive/splits_bmo/splits.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Étape 4 — calibrate_tau_and_run_gate"
      ],
      "metadata": {
        "id": "zmKG53TG_y4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Étape 4 — Calibration τ + exécution du protocole (BMO)\n",
        "\n",
        "- Lit les splits BMO : episodes_dev(.clean).jsonl, episodes_holdoutH(.clean).jsonl\n",
        "- Calcule un score p_top (proxy déterministe, anti-leakage)\n",
        "- Sweep τ sur DEV et H : abstention_rate et coverage_on_labeled\n",
        "- Choisit τ* pour viser une abstention cible sur DEV (par ex. 20 %)\n",
        "- Relance le protocole avec τ* sur DEV et H :\n",
        "  -> eligibility_audit.csv\n",
        "  -> figures (distribution décisions, p_top, latence)\n",
        "  -> traces/ et prov/\n",
        "\n",
        "Adapte seulement les chemins si besoin.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import csv\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "# ==================== CONFIG ====================\n",
        "\n",
        "# Chemins d'entrée (on préfère *.clean.jsonl s'ils existent)\n",
        "DEV_CANDIDATES = [\n",
        "    \"/content/drive/MyDrive/splits_bmo/episodes_dev.clean.jsonl\",\n",
        "    \"/content/drive/MyDrive/splits_bmo/episodes_dev.jsonl\",\n",
        "]\n",
        "\n",
        "H_CANDIDATES = [\n",
        "    \"/content/drive/MyDrive/splits_bmo/episodes_holdoutH.clean.jsonl\",\n",
        "    \"/content/drive/MyDrive/splits_bmo/episodes_holdoutH.jsonl\",\n",
        "]\n",
        "\n",
        "# Dossiers de sortie\n",
        "SWEEP_DIR = \"/content/drive/MyDrive/sweep_bmo\"\n",
        "OUT_DEV   = \"/content/drive/MyDrive/protocol_bmo_dev\"\n",
        "OUT_H     = \"/content/drive/MyDrive/protocol_bmo_H\"\n",
        "\n",
        "# Cible d'abstention (20 %)\n",
        "TARGET_ABST = 0.20\n",
        "\n",
        "# Labels d'incidents (ceux qu'on considère \"intéressants\")\n",
        "I_IDS = {\n",
        "    \"i:release_regression\",\n",
        "    \"i:infra_instability\",\n",
        "    \"i:security_threat\",\n",
        "    \"i:minor_degradation\",\n",
        "    \"i:insufficient_info\",\n",
        "    \"i:false_positive\",\n",
        "    \"i:business_side_effect\",\n",
        "}\n",
        "\n",
        "# =================================================\n",
        "\n",
        "def first_existing(paths):\n",
        "    for p in paths:\n",
        "        if os.path.exists(p):\n",
        "            return p\n",
        "    return paths[0]\n",
        "\n",
        "def ensure_dir(p):\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "def load_jsonl(path):\n",
        "    items = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            try:\n",
        "                items.append(json.loads(line))\n",
        "            except Exception:\n",
        "                continue\n",
        "    return items\n",
        "\n",
        "# ---- proxy p_top (déterministe, anti-leakage) ----\n",
        "\n",
        "def sigmoid(x):\n",
        "    try:\n",
        "        return 1.0 / (1.0 + math.exp(-x))\n",
        "    except OverflowError:\n",
        "        return 0.0 if x < 0 else 1.0\n",
        "\n",
        "def infer_p_top(card):\n",
        "    \"\"\"\n",
        "    Score de confiance p_top basé uniquement sur :\n",
        "      - summary_len\n",
        "      - text_len\n",
        "      - severity\n",
        "    Pas de status ni de résolution -> pas de fuite.\n",
        "    \"\"\"\n",
        "    s_len = int(card.get(\"summary_len\", 0) or 0)\n",
        "    t_len = int(card.get(\"text_len\", 0) or 0)\n",
        "    sev   = (card.get(\"severity\") or \"unknown\").lower()\n",
        "\n",
        "    sev_bias = {\"high\": 0.6, \"medium\": 0.35, \"low\": 0.15}.get(sev, 0.25)\n",
        "\n",
        "    x = 0.02 * min(s_len, 200) + 0.005 * min(t_len, 1200) + sev_bias\n",
        "\n",
        "    # léger jitter déterministe par clé (pour ne pas avoir tous les mêmes)\n",
        "    k = str(card.get(\"key\") or card.get(\"ticket_id\") or \"\")\n",
        "    jitter = (hash(k) % 1000) / 1000.0  # [0, 1)\n",
        "    x = x + 0.10 * (jitter - 0.5)\n",
        "\n",
        "    return max(0.0, min(1.0, sigmoid(x)))\n",
        "\n",
        "# ---- figures (matplotlib, sans style/couleurs forcés) ----\n",
        "\n",
        "def _mpl():\n",
        "    import matplotlib\n",
        "    matplotlib.use(\"Agg\")\n",
        "    import matplotlib.pyplot as plt\n",
        "    return plt\n",
        "\n",
        "def fig_abstention_vs_tau(taus, abst, title, out_png):\n",
        "    plt = _mpl()\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.plot(taus, abst, marker=\"o\")\n",
        "    plt.xlabel(\"tau\")\n",
        "    plt.ylabel(\"abstention_rate\")\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_png, dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "def fig_coverage_vs_tau(taus, cov, title, out_png):\n",
        "    plt = _mpl()\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.plot(taus, cov, marker=\"o\")\n",
        "    plt.xlabel(\"tau\")\n",
        "    plt.ylabel(\"coverage_on_labeled\")\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_png, dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "def fig_decision_distribution(accepted_mask, out_png):\n",
        "    plt = _mpl()\n",
        "    acc = sum(1 for a in accepted_mask if a)\n",
        "    abst = len(accepted_mask) - acc\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.bar([\"accepted\", \"abstain\"], [acc, abst])\n",
        "    plt.title(\"Decision distribution\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_png, dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "def fig_p_top_hist_all(p_tops, out_png):\n",
        "    plt = _mpl()\n",
        "    plt.figure(figsize=(8,4.5))\n",
        "    plt.hist(p_tops, bins=30)\n",
        "    plt.xlabel(\"p_top\")\n",
        "    plt.ylabel(\"count\")\n",
        "    plt.title(\"p_top (all)\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_png, dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "def fig_p_top_hist_accepted(p_tops, accepted_mask, out_png):\n",
        "    vals = [v for v,a in zip(p_tops, accepted_mask) if a]\n",
        "    plt = _mpl()\n",
        "    plt.figure(figsize=(8,4.5))\n",
        "    plt.hist(vals, bins=30)\n",
        "    plt.xlabel(\"p_top (accepted)\")\n",
        "    plt.ylabel(\"count\")\n",
        "    plt.title(\"p_top (accepted only)\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_png, dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "def fig_latency_hist(latencies_ms, out_png):\n",
        "    plt = _mpl()\n",
        "    plt.figure(figsize=(8,4.5))\n",
        "    plt.hist(latencies_ms, bins=30)\n",
        "    plt.xlabel(\"latency (ms)\")\n",
        "    plt.ylabel(\"count\")\n",
        "    plt.title(\"Latency histogram\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_png, dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "# ---- métriques pour un τ donné ----\n",
        "\n",
        "def metrics_for_tau(cards, tau):\n",
        "    \"\"\"\n",
        "    Retourne:\n",
        "      - abstention_rate_all\n",
        "      - coverage_on_labeled (sur les cartes avec silver_label dans I_IDS)\n",
        "    \"\"\"\n",
        "    accepted_flags = []\n",
        "    labeled_flags = []\n",
        "    accepted_and_labeled = []\n",
        "\n",
        "    for c in cards:\n",
        "        p = infer_p_top(c)\n",
        "        accept = (p >= (1.0 - tau))\n",
        "        accepted_flags.append(accept)\n",
        "\n",
        "        lab = c.get(\"_silver_label\") or \"NONE\"\n",
        "        is_labeled = lab in I_IDS\n",
        "\n",
        "        labeled_flags.append(is_labeled)\n",
        "        accepted_and_labeled.append(accept and is_labeled)\n",
        "\n",
        "    n = len(cards)\n",
        "    abst_all = 1.0 - (sum(1 for a in accepted_flags if a) / n) if n else 0.0\n",
        "\n",
        "    n_labeled = sum(1 for x in labeled_flags if x)\n",
        "    cov = (sum(1 for x in accepted_and_labeled if x) / n_labeled) if n_labeled else 0.0\n",
        "\n",
        "    return abst_all, cov\n",
        "\n",
        "# ---- sweep τ ----\n",
        "\n",
        "def sweep_tau(cards, label, out_dir):\n",
        "    ensure_dir(out_dir)\n",
        "    taus = [i / 200.0 for i in range(0, 101)]  # 0.00 -> 0.50\n",
        "    abst_list = []\n",
        "    cov_list = []\n",
        "\n",
        "    csv_path = os.path.join(out_dir, f\"sweep_{label}.csv\")\n",
        "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as fw:\n",
        "        cw = csv.writer(fw)\n",
        "        cw.writerow([\"tau\", \"abstention_rate_all\", \"coverage_on_labeled\"])\n",
        "\n",
        "        for t in taus:\n",
        "            abst, cov = metrics_for_tau(cards, t)\n",
        "            abst_list.append(abst)\n",
        "            cov_list.append(cov)\n",
        "            cw.writerow([f\"{t:.4f}\", f\"{abst:.4f}\", f\"{cov:.4f}\"])\n",
        "\n",
        "    # Figures\n",
        "    fig_abstention_vs_tau(\n",
        "        taus, abst_list,\n",
        "        f\"Abstention vs tau ({label})\",\n",
        "        os.path.join(out_dir, f\"fig_abstention_vs_tau_{label}.png\"),\n",
        "    )\n",
        "    fig_coverage_vs_tau(\n",
        "        taus, cov_list,\n",
        "        f\"Coverage-on-labeled vs tau ({label})\",\n",
        "        os.path.join(out_dir, f\"fig_coverage_vs_tau_{label}.png\"),\n",
        "    )\n",
        "\n",
        "    return taus, abst_list, cov_list\n",
        "\n",
        "# ---- exécution protocole pour un τ donné ----\n",
        "\n",
        "def run_protocol(cards, tau, out_dir):\n",
        "    ensure_dir(out_dir)\n",
        "    os.makedirs(os.path.join(out_dir, \"traces\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(out_dir, \"prov\"), exist_ok=True)\n",
        "\n",
        "    audit_csv = os.path.join(out_dir, \"eligibility_audit.csv\")\n",
        "\n",
        "    p_tops = []\n",
        "    accepted_mask = []\n",
        "    latencies = []\n",
        "\n",
        "    with open(audit_csv, \"w\", newline=\"\", encoding=\"utf-8\") as fw:\n",
        "        cw = csv.writer(fw)\n",
        "        cw.writerow([\"episode_id\", \"p_top\", \"decision\", \"accepted\"])\n",
        "\n",
        "        for c in cards:\n",
        "            p = infer_p_top(c)\n",
        "            accept = (p >= (1.0 - tau))\n",
        "\n",
        "            # Latence proxy (en ms) : fonction de text_len + petit bruit\n",
        "            lat = 10.0 + 0.02 * (int(c.get(\"text_len\", 0) or 0)) + (\n",
        "                hash(str(c.get(\"key\", \"\"))) % 7\n",
        "            )\n",
        "\n",
        "            p_tops.append(p)\n",
        "            accepted_mask.append(accept)\n",
        "            latencies.append(lat)\n",
        "\n",
        "            eid = str(c.get(\"key\") or c.get(\"ticket_id\") or \"\")\n",
        "            cw.writerow([eid, f\"{p:.4f}\", \"accept\" if accept else \"abstain\", int(accept)])\n",
        "\n",
        "            trace = {\n",
        "                \"episode_id\": eid,\n",
        "                \"features\": {\n",
        "                    \"summary_len\": int(c.get(\"summary_len\", 0) or 0),\n",
        "                    \"text_len\": int(c.get(\"text_len\", 0) or 0),\n",
        "                    \"severity\": c.get(\"severity\") or \"unknown\",\n",
        "                },\n",
        "                \"p_top\": p,\n",
        "                \"tau\": tau,\n",
        "                \"decision\": \"accept\" if accept else \"abstain\",\n",
        "            }\n",
        "            prov = {\n",
        "                \"episode_id\": eid,\n",
        "                \"generated_by\": \"step4_bmo\",\n",
        "                \"inputs\": {},\n",
        "                \"params\": {\"tau\": tau},\n",
        "            }\n",
        "\n",
        "            with open(os.path.join(out_dir, \"traces\", f\"{eid}.json\"), \"w\", encoding=\"utf-8\") as ft:\n",
        "                json.dump(trace, ft, ensure_ascii=False, indent=2)\n",
        "            with open(os.path.join(out_dir, \"prov\", f\"{eid}.json\"), \"w\", encoding=\"utf-8\") as fp:\n",
        "                json.dump(prov, fp, ensure_ascii=False, indent=2)\n",
        "\n",
        "    abst = 1.0 - (sum(1 for a in accepted_mask if a) / len(accepted_mask))\n",
        "    print(f\"[RUN] episodes: {len(cards)}\")\n",
        "    print(f\"[RUN] abstention_rate: {abst:.4f}\")\n",
        "    print(f\"- OUT_DIR: {out_dir}\")\n",
        "    print(\"- eligibility_audit.csv prêt\")\n",
        "\n",
        "    # Figures\n",
        "    fig_decision_distribution(accepted_mask, os.path.join(out_dir, \"fig_decision_distribution.png\"))\n",
        "    fig_p_top_hist_all(p_tops, os.path.join(out_dir, \"fig_p_top_hist_all.png\"))\n",
        "    fig_p_top_hist_accepted(p_tops, accepted_mask, os.path.join(out_dir, \"fig_p_top_hist_accepted.png\"))\n",
        "    fig_latency_hist(latencies, os.path.join(out_dir, \"fig_latency_hist.png\"))\n",
        "\n",
        "    print(f\"[RUN] traces: {len(cards)}\")\n",
        "    print(f\"[RUN] prov: {len(cards)}\")\n",
        "\n",
        "# ---- main ----\n",
        "\n",
        "def main():\n",
        "    dev_path = first_existing(DEV_CANDIDATES)\n",
        "    h_path   = first_existing(H_CANDIDATES)\n",
        "\n",
        "    dev_cards = load_jsonl(dev_path)\n",
        "    h_cards   = load_jsonl(h_path)\n",
        "\n",
        "    print(f\"[LOAD] DEV: {len(dev_cards)} episodes\")\n",
        "    print(f\"[LOAD] H  : {len(h_cards)} episodes\")\n",
        "\n",
        "    # 1) Sweep τ\n",
        "    ensure_dir(SWEEP_DIR)\n",
        "\n",
        "    print(\"\\n[SWEEP] DEV\")\n",
        "    taus, abst_dev, cov_dev = sweep_tau(dev_cards, \"DEV\", SWEEP_DIR)\n",
        "\n",
        "    print(\"\\n[SWEEP] H\")\n",
        "    _, abst_h, cov_h = sweep_tau(h_cards, \"H\", SWEEP_DIR)\n",
        "\n",
        "    # 2) Choix τ* (sur DEV, cible d'abstention)\n",
        "    best_idx = min(range(len(taus)), key=lambda i: abs(abst_dev[i] - TARGET_ABST))\n",
        "    tau_star = taus[best_idx]\n",
        "    thr = 1.0 - tau_star\n",
        "\n",
        "    print(f\"\\n[CAL] target_abst={TARGET_ABST:.2f} -> τ*={tau_star:.4f} (thr={thr:.4f})\")\n",
        "\n",
        "    # 3) Run protocole avec τ*\n",
        "    print(\"\\n== DEV ==\")\n",
        "    run_protocol(dev_cards, tau_star, OUT_DEV)\n",
        "\n",
        "    print(\"\\n== HOLDOUT-H ==\")\n",
        "    run_protocol(h_cards, tau_star, OUT_H)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMrSL7Re_2ar",
        "outputId": "257fb810-7202-4a00-d479-da7f6008a51b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOAD] DEV: 157 episodes\n",
            "[LOAD] H  : 43 episodes\n",
            "\n",
            "[SWEEP] DEV\n",
            "\n",
            "[SWEEP] H\n",
            "\n",
            "[CAL] target_abst=0.20 -> τ*=0.2400 (thr=0.7600)\n",
            "\n",
            "== DEV ==\n",
            "[RUN] episodes: 157\n",
            "[RUN] abstention_rate: 0.1975\n",
            "- OUT_DIR: /content/drive/MyDrive/protocol_bmo_dev\n",
            "- eligibility_audit.csv prêt\n",
            "[RUN] traces: 157\n",
            "[RUN] prov: 157\n",
            "\n",
            "== HOLDOUT-H ==\n",
            "[RUN] episodes: 43\n",
            "[RUN] abstention_rate: 0.1395\n",
            "- OUT_DIR: /content/drive/MyDrive/protocol_bmo_H\n",
            "- eligibility_audit.csv prêt\n",
            "[RUN] traces: 43\n",
            "[RUN] prov: 43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Étape 5 — evaluate_gate_vs_gold"
      ],
      "metadata": {
        "id": "k17PUrGhDAOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Étape 5 — Évaluation de la PORTE (gating-only) vs GOLD — BMO\n",
        "\n",
        "- Joint : eligibility_audit.csv (accept/abstain) × gold_labels.csv\n",
        "- Mesure :\n",
        "    * abstention_rate_all  (sur tous les tickets)\n",
        "    * coverage_on_labeled  (part des tickets *labellisés* que la porte accepte)\n",
        "    * coverage_per_label   (même chose, par type d'incident)\n",
        "- Sorties :\n",
        "    * protocol_bmo_eval_DEV/metrics_gate.json + figures\n",
        "    * protocol_bmo_eval_H/metrics_gate.json   + figures\n",
        "\"\"\"\n",
        "\n",
        "import os, csv, json\n",
        "\n",
        "# ========= CHEMINS À VÉRIFIER (BMO) =========\n",
        "GOLD    = \"/content/drive/MyDrive/gold/gold_labels.csv\"\n",
        "AUD_DEV = \"/content/drive/MyDrive/protocol_bmo_dev/eligibility_audit.csv\"\n",
        "AUD_H   = \"/content/drive/MyDrive/protocol_bmo_H/eligibility_audit.csv\"\n",
        "\n",
        "OUT_DEV = \"/content/drive/MyDrive/protocol_bmo_eval_DEV\"\n",
        "OUT_H   = \"/content/drive/MyDrive/protocol_bmo_eval_H\"\n",
        "# ===========================================\n",
        "\n",
        "# Les incidents qu'on considère (doivent correspondre aux labels GOLD)\n",
        "I_IDS = {\n",
        "    \"i:release_regression\",\n",
        "    \"i:infra_instability\",\n",
        "    \"i:security_threat\",\n",
        "    \"i:minor_degradation\",\n",
        "    \"i:insufficient_info\",\n",
        "    \"i:false_positive\",\n",
        "    \"i:business_side_effect\",\n",
        "}\n",
        "\n",
        "def ensure_dir(p: str):\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "def load_gold_map(path: str):\n",
        "    \"\"\"\n",
        "    Charge gold_labels.csv -> dict episode_id -> gold_label\n",
        "    et ajoute les variantes BUGS-12345 / 12345 pour être robuste.\n",
        "    \"\"\"\n",
        "    m = {}\n",
        "    with open(path, newline=\"\", encoding=\"utf-8\") as f:\n",
        "        rdr = csv.DictReader(f)\n",
        "        for r in rdr:\n",
        "            eid = str(r.get(\"episode_id\", \"\")).strip()\n",
        "            lab = str(r.get(\"gold_label\", \"\")).strip()\n",
        "            if not eid or lab not in I_IDS:\n",
        "                continue\n",
        "            m[eid] = lab\n",
        "            # mapping BUGS-12345 <-> 12345\n",
        "            if eid.startswith(\"BUGS-\"):\n",
        "                num = eid.split(\"-\", 1)[1]\n",
        "                m[num] = lab\n",
        "            else:\n",
        "                m[f\"BUGS-{eid}\"] = lab\n",
        "    return m\n",
        "\n",
        "def eval_gate(audit_csv: str, gold_map: dict, out_dir: str):\n",
        "    \"\"\"\n",
        "    Lit eligibility_audit.csv et joint avec gold_map.\n",
        "    Calcule métriques + figures simples.\n",
        "    \"\"\"\n",
        "    ensure_dir(out_dir)\n",
        "    rows = []\n",
        "    with open(audit_csv, newline=\"\", encoding=\"utf-8\") as f:\n",
        "        rdr = csv.DictReader(f)\n",
        "        for r in rdr:\n",
        "            eid = r[\"episode_id\"]\n",
        "            p   = float(r[\"p_top\"])\n",
        "            acc = (r[\"accepted\"] in {\"1\", \"True\", \"true\", \"TRUE\"})\n",
        "            lab = gold_map.get(eid)\n",
        "            rows.append((eid, p, acc, lab))\n",
        "\n",
        "    n_all = len(rows)\n",
        "    labeled = [(eid, p, acc, lab) for (eid, p, acc, lab) in rows if lab is not None]\n",
        "    n_lab = len(labeled)\n",
        "    covered = [(eid, p, acc, lab) for (eid, p, acc, lab) in labeled if acc]\n",
        "    n_cov = len(covered)\n",
        "\n",
        "    # taux d’abstention global (sur tous les tickets de la split)\n",
        "    abst_all = 1.0 - (sum(1 for (_, _, acc, _) in rows if acc) / n_all) if n_all else 0.0\n",
        "    # parmi les tickets avec étiquette GOLD, combien sont passés à travers la porte ?\n",
        "    coverage_on_labeled = (n_cov / n_lab) if n_lab else 0.0\n",
        "\n",
        "    # couverture par type d’incident (GOLD)\n",
        "    per_label_tot = {lab: 0 for lab in I_IDS}\n",
        "    per_label_cov = {lab: 0 for lab in I_IDS}\n",
        "    for (_, _, acc, lab) in labeled:\n",
        "        per_label_tot[lab] += 1\n",
        "        if acc:\n",
        "            per_label_cov[lab] += 1\n",
        "    coverage_per_label = {\n",
        "        lab: (per_label_cov[lab] / per_label_tot[lab]) if per_label_tot[lab] > 0 else 0.0\n",
        "        for lab in I_IDS\n",
        "    }\n",
        "\n",
        "    metrics = {\n",
        "        \"episodes_all\": n_all,\n",
        "        \"accepted_all\": sum(1 for (_, _, acc, _) in rows if acc),\n",
        "        \"abstention_rate_all\": round(abst_all, 4),\n",
        "        \"episodes_labeled\": n_lab,\n",
        "        \"covered_non_abstain\": n_cov,\n",
        "        \"coverage_on_labeled\": round(coverage_on_labeled, 4),\n",
        "        \"coverage_per_label\": {k: round(v, 4) for k, v in coverage_per_label.items()},\n",
        "    }\n",
        "\n",
        "    # Sauvegarde JSON\n",
        "    with open(os.path.join(out_dir, \"metrics_gate.json\"), \"w\", encoding=\"utf-8\") as fw:\n",
        "        json.dump(metrics, fw, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # Figures simples (matplotlib sans style particulier)\n",
        "    try:\n",
        "        import matplotlib\n",
        "        matplotlib.use(\"Agg\")\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        # 1) barre couverture\n",
        "        plt.figure(figsize=(5, 4))\n",
        "        plt.bar([\"labeled\", \"covered_non_abstain\"], [n_lab, n_cov])\n",
        "        plt.title(\"Gate coverage\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(out_dir, \"fig_gate_coverage.png\"), dpi=160)\n",
        "        plt.close()\n",
        "\n",
        "        # 2) distribution p_top sur les couverts\n",
        "        pts = [p for (_, p, acc, lab) in covered]\n",
        "        plt.figure(figsize=(8, 4.5))\n",
        "        if pts:\n",
        "            plt.hist(pts, bins=30)\n",
        "        plt.xlabel(\"p_top (covered & non-abstain)\")\n",
        "        plt.ylabel(\"count\")\n",
        "        plt.title(\"p_top on covered tickets\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(out_dir, \"fig_p_top_on_covered.png\"), dpi=160)\n",
        "        plt.close()\n",
        "    except Exception as e:\n",
        "        print(\"[FIG][WARN]\", e)\n",
        "\n",
        "    print(f\"[EVAL] {out_dir} -> {metrics}\")\n",
        "\n",
        "def main():\n",
        "    gold = load_gold_map(GOLD)\n",
        "    print(f\"[INFO] GOLD chargés : {len(gold)} épisodes\")\n",
        "\n",
        "    # DEV\n",
        "    eval_gate(AUD_DEV, gold, OUT_DEV)\n",
        "\n",
        "    # HOLDOUT-H\n",
        "    eval_gate(AUD_H, gold, OUT_H)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8mA7TFPEJ6P",
        "outputId": "71de8d6c-e011-4036-de4d-9e4a3ad681fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] GOLD chargés : 400 épisodes\n",
            "[EVAL] /content/drive/MyDrive/protocol_bmo_eval_DEV -> {'episodes_all': 157, 'accepted_all': 126, 'abstention_rate_all': 0.1975, 'episodes_labeled': 157, 'covered_non_abstain': 126, 'coverage_on_labeled': 0.8025, 'coverage_per_label': {'i:insufficient_info': 0.822, 'i:release_regression': 0.0, 'i:security_threat': 0.0, 'i:infra_instability': 0.0, 'i:false_positive': 0.7391, 'i:minor_degradation': 1.0, 'i:business_side_effect': 0.7333}}\n",
            "[EVAL] /content/drive/MyDrive/protocol_bmo_eval_H -> {'episodes_all': 43, 'accepted_all': 37, 'abstention_rate_all': 0.1395, 'episodes_labeled': 43, 'covered_non_abstain': 37, 'coverage_on_labeled': 0.8605, 'coverage_per_label': {'i:insufficient_info': 0.9032, 'i:release_regression': 0.0, 'i:security_threat': 0.0, 'i:infra_instability': 1.0, 'i:false_positive': 0.8333, 'i:minor_degradation': 0.0, 'i:business_side_effect': 0.75}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2XP_5pceQKYL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}